{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of Spark Connect (Apache Spark 4.0.0-SNAPSHOT)","text":"<p>Welcome to The Internals of Spark Connect online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, a Freelance Data(bricks) Engineer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams) with brief forays into a wider data engineering space (mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Connect as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Spark Connect \ud83d\udd25</p> <p>Last update: 2024-12-22</p>"},{"location":"logging/","title":"Logging","text":"<p>Spark Connect uses the same logging infrastructure as Apache Spark.</p>"},{"location":"overview/","title":"Spark Connect","text":"<p>Spark Connect is a client-server interface for Apache Spark for remote connectivity to Spark clusters (using the DataFrame API and unresolved logical plans as the protocol based on gRPC Java).</p> Apache Spark 3.4 <p>Spark Connect has been available since Apache Spark 3.4.</p> <p>The Spark Connect server can be started using <code>sbin/start-connect-server.sh</code> shell script.</p> <pre><code>$ ./sbin/start-connect-server.sh\nstarting org.apache.spark.sql.connect.service.SparkConnectServer, logging to...\n\n$ tail -1 logs/spark-jacek-org.apache.spark.sql.connect.service.SparkConnectServer-1-Jaceks-Mac-mini.local.out\n... Spark Connect server started.\n</code></pre> <p>Use Spark Connect for interactive analysis:</p> <pre><code>./bin/pyspark --remote \"sc://localhost\"\n</code></pre> <p>And you will notice that the PySpark shell welcome message tells you that you have connected to Spark using Spark Connect:</p> <pre><code>Client connected to the Spark Connect server at localhost\n</code></pre> <p>Check the Spark session type:</p> <pre><code>SparkSession available as 'spark'.\n&gt;&gt;&gt; type(spark)\n&lt;class 'pyspark.sql.connect.session.SparkSession'&gt;\n</code></pre> <p>Now you can run PySpark code in the shell to see Spark Connect in action:</p> <pre><code>&gt;&gt;&gt; columns = [\"id\",\"name\"]\n&gt;&gt;&gt; data = [(1,\"Sarah\"),(2,\"Maria\")]\n&gt;&gt;&gt; df = spark.createDataFrame(data).toDF(*columns)\n&gt;&gt;&gt; df.show()\n+---+-----+\n| id| name|\n+---+-----+\n|  1|Sarah|\n|  2|Maria|\n+---+-----+\n</code></pre>"},{"location":"client/","title":"Spark Connect Client","text":"<p>Spark Connect Client can be started using...FIXME</p>"},{"location":"client/Builder/","title":"SparkConnectClient.Builder","text":"<p><code>SparkConnectClient.Builder</code> is a factory to create a SparkConnectClient.</p>"},{"location":"client/Builder/#build","title":"Create SparkConnectClient","text":"<pre><code>build(): SparkConnectClient\n</code></pre> <p><code>build</code> requests this Configuration for a SparkConnectClient</p> <p><code>build</code> is used when:</p> <ul> <li><code>ConnectRepl</code> standalone application is launched on command line</li> </ul>"},{"location":"client/Configuration/","title":"Configuration","text":""},{"location":"client/Configuration/#toSparkConnectClient","title":"toSparkConnectClient","text":"<pre><code>toSparkConnectClient: SparkConnectClient\n</code></pre> <p><code>toSparkConnectClient</code> creates a new SparkConnectClient for this <code>Configuration</code> and a new ManagedChannel.</p> <p><code>toSparkConnectClient</code> is used when:</p> <ul> <li><code>SparkSession</code> (Spark SQL) is requested to create a <code>SparkSession</code></li> <li><code>SparkConnectClient</code> is requested to clone itself</li> <li><code>SparkConnectClient.Builder</code> is requested to build a SparkConnectClient</li> </ul>"},{"location":"client/CustomSparkConnectBlockingStub/","title":"CustomSparkConnectBlockingStub","text":""},{"location":"client/CustomSparkConnectBlockingStub/#creating-instance","title":"Creating Instance","text":"<p><code>CustomSparkConnectBlockingStub</code> takes the following to be created:</p> <ul> <li> <code>ManagedChannel</code> <li> <code>SparkConnectStubState</code> <p><code>CustomSparkConnectBlockingStub</code> is created along with SparkConnectClient.</p>"},{"location":"client/SparkConnectClient/","title":"SparkConnectClient","text":""},{"location":"client/SparkConnectClient/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectClient</code> takes the following to be created:</p> <ul> <li> Configuration <li> <code>ManagedChannel</code> (gRPC) <p><code>SparkConnectClient</code> is created when:</p> <ul> <li><code>Configuration</code> is requested for a SparkConnectClient</li> </ul>"},{"location":"client/SparkConnectClient/#bstub","title":"CustomSparkConnectBlockingStub","text":"<p><code>SparkConnectClient</code> creates a new CustomSparkConnectBlockingStub when created with the following:</p> CustomSparkConnectBlockingStub Value ManagedChannel This ManagedChannel SparkConnectStubState This SparkConnectStubState <p>This <code>CustomSparkConnectBlockingStub</code> is used for the following:</p> <ul> <li>Analyze a plan</li> <li>Config</li> <li>Execute a plan</li> <li>Interrupt All</li> <li>Interrupt Operation</li> <li>Interrupt by Tag</li> <li>Release Session</li> <li>Create this ArtifactManager</li> </ul>"},{"location":"client/SparkConnectClient/#copy","title":"Clone Itself","text":"<pre><code>copy(): SparkConnectClient\n</code></pre> <p><code>copy</code> requests this Configuration for a SparkConnectClient</p> <p><code>copy</code> is used when:</p> <ul> <li><code>SparkSession</code> (Spark SQL) is requested for a new <code>SparkSession</code></li> </ul>"},{"location":"client/SparkConnectClient/#releaseSession","title":"Release Session","text":"<pre><code>releaseSession(): proto.ReleaseSessionResponse\n</code></pre> <p><code>releaseSession</code> builds a new <code>ReleaseSessionRequest</code> with the following:</p> <ul> <li>This UserContext</li> <li>This sessionId</li> <li>This userAgent</li> </ul> <p>In the end, <code>releaseSession</code> requests this CustomSparkConnectBlockingStub to releaseSession with the <code>ReleaseSessionRequest</code>.</p> <p><code>releaseSession</code> is used when:</p> <ul> <li><code>SparkSession</code> is requested to close</li> </ul>"},{"location":"jvm/","title":"JVM Client","text":""},{"location":"jvm/ConnectRepl/","title":"ConnectRepl \u2014 Spark Connect REPL","text":"<p><code>ConnectRepl</code> is a standalone application that can be launched on command line using the following shell scripts:</p> <ul> <li><code>bin/spark-connect-shell</code></li> <li><code>connector/connect/bin/spark-connect-scala-client</code></li> </ul> <p><code>ConnectRepl</code> uses Ammonite REPL for REPL experience.</p>"},{"location":"jvm/ConnectRepl/#main","title":"Launching Application","text":"<pre><code>main(\n  args: Array[String]): Unit\n</code></pre> <p><code>main</code> runs a new Spark Connect Server to connect locally.</p> <p><code>main</code> builds a SparkConnectClient.</p> <p><code>main</code> builds a <code>SparkSession</code> for the <code>SparkConnectClient</code>.</p>"},{"location":"jvm/DataFrameWriterImpl/","title":"DataFrameWriterImpl","text":"<p><code>DataFrameWriterImpl</code> is...FIXME</p>"},{"location":"jvm/DataFrameWriterV2Impl/","title":"DataFrameWriterV2Impl","text":"<p><code>DataFrameWriterV2Impl</code> is...FIXME</p>"},{"location":"jvm/DataStreamWriter/","title":"DataStreamWriter","text":"<p><code>DataStreamWriter</code> is...FIXME</p>"},{"location":"jvm/Dataset/","title":"Dataset","text":"<p><code>Dataset</code> is...FIXME</p>"},{"location":"jvm/MergeIntoWriterImpl/","title":"MergeIntoWriterImpl","text":"<p><code>MergeIntoWriterImpl</code> is...FIXME</p>"},{"location":"jvm/RemoteStreamingQuery/","title":"RemoteStreamingQuery","text":"<p><code>RemoteStreamingQuery</code> is...FIXME</p>"},{"location":"jvm/SessionCleaner/","title":"SessionCleaner","text":"<p><code>SessionCleaner</code> is...FIXME</p>"},{"location":"jvm/SparkSession.Builder/","title":"SparkSession.Builder","text":""},{"location":"jvm/SparkSession.Builder/#create","title":"Create SparkSession","text":"<pre><code>create(): SparkSession\n</code></pre> <p><code>create</code> withLocalConnectServer.</p> <p><code>create</code>...FIXME</p>"},{"location":"jvm/SparkSession/","title":"SparkSession \u2014 Spark Connect Scala Client","text":"<p><code>SparkSession</code> is a Spark Connect Scala Client.</p> <p><code>SparkSession</code> is a <code>api.SparkSession</code>.</p> Spark SQL's SparkSession <p>There is the Spark SQL variant of <code>api.SparkSession</code>.</p>"},{"location":"jvm/SparkSession/#creating-instance","title":"Creating Instance","text":"<p><code>SparkSession</code> takes the following to be created:</p> <ul> <li> SparkConnectClient <li> Plan ID Generator (<code>AtomicLong</code>) <p><code>SparkSession</code> is created when:</p> <ul> <li><code>SparkSession</code> utility is used to create a SparkSession (for a Configuration)</li> <li><code>SparkSession.Builder</code> is requested to tryCreateSessionFromClient</li> </ul>"},{"location":"jvm/SparkSession/#create","title":"Create SparkSession","text":"<pre><code>create(\n  configuration: Configuration): SparkSession\n</code></pre> <p><code>create</code> creates a new SparkSession for a SparkConnectClient (based on the given Configuration) and this Plan ID Generator.</p> <p><code>create</code> is used when:</p> <ul> <li><code>SparkSession</code> utility is used to load a SparkSession from a cache</li> <li><code>SparkSession.Builder</code> is requested to create a SparkSession</li> </ul>"},{"location":"jvm/SparkSession/#execute","title":"Execute Command","text":"<pre><code>execute(\n  command: proto.Command): Seq[ExecutePlanResponse]\n</code></pre> <p>DeveloperApi</p> <p><code>execute</code> is a <code>DeveloperApi</code>.</p> <p><code>execute</code> executeInternal a new <code>proto.Plan</code> for the given <code>proto.Command</code>.</p> <p><code>execute</code> is used when:</p> <ul> <li><code>Dataset</code> is requested to checkpoint and createTempView</li> <li><code>SparkSession</code> is requested to registerUdf</li> <li><code>DataFrameWriterImpl</code> is requested to executeWriteOperation</li> <li><code>DataFrameWriterV2Impl</code> is requested to executeWriteOperation</li> <li><code>MergeIntoWriterImpl</code> is requested to merge</li> <li><code>SessionCleaner</code> is requested to doCleanupCachedRemoteRelation</li> <li><code>DataStreamWriter</code> is requested to start</li> <li><code>RemoteStreamingQuery</code> is requested to executeQueryCmd</li> <li><code>StreamingQueryListenerBus</code> is requested to <code>remove</code> a <code>StreamingQueryListener</code></li> <li><code>StreamingQueryManager</code> is requested to <code>executeManagerCmd</code></li> </ul>"},{"location":"jvm/SparkSession/#executeInternal","title":"executeInternal","text":"<pre><code>executeInternal(\n  plan: proto.Plan): CloseableIterator[ExecutePlanResponse]\n</code></pre> <p><code>executeInternal</code> requests the SparkConnectClient to execute the given <code>proto.Plan</code>.</p> <p>With an <code>ExecutePlanResponse</code>, <code>executeInternal</code> processRegisteredObservedMetrics.</p>"},{"location":"jvm/SparkSession/#withLocalConnectServer","title":"withLocalConnectServer","text":"<pre><code>withLocalConnectServer[T](\n  f: =&gt; T): T\n</code></pre> <p><code>withLocalConnectServer</code>...FIXME</p> <p><code>withLocalConnectServer</code> is used when:</p> <ul> <li><code>SparkSession.Builder</code> is requested to create a SparkSession and getOrCreate</li> <li>ConnectRepl standalone application is launched</li> </ul>"},{"location":"server/","title":"Spark Connect Server","text":"<p>Spark Connect Server is a gRPC service that can be started for the following environments:</p> <ul> <li>Apache Spark applications (as a Spark driver plugin)</li> <li>On command line using sbin/start-connect-server.sh utility</li> </ul>"},{"location":"server/CommandPlugin/","title":"CommandPlugin","text":"<p><code>CommandPlugin</code> is an abstraction of command processors in Spark Connect that can process a command.</p>"},{"location":"server/CommandPlugin/#contract","title":"Contract","text":""},{"location":"server/CommandPlugin/#process","title":"Process Command","text":"<pre><code>process(\n  command: protobuf.Any,\n  planner: SparkConnectPlanner): Option[Unit]\n</code></pre> <p>Used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to handleCommandPlugin</li> </ul>"},{"location":"server/CommandPlugin/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"server/ExecuteHolder/","title":"ExecuteHolder","text":""},{"location":"server/ExecuteHolder/#creating-instance","title":"Creating Instance","text":"<p><code>ExecuteHolder</code> takes the following to be created:</p> <ul> <li> <code>ExecutePlanRequest</code> <li> <code>SessionHolder</code> <p><code>ExecuteHolder</code> is created when:</p> <ul> <li><code>SparkConnectExecutionManager</code> is requested to create an ExecuteHolder</li> </ul>"},{"location":"server/ExecuteThreadRunner/","title":"ExecuteThreadRunner","text":"<p><code>ExecuteThreadRunner</code> is created alongside ExecuteHolder.</p>"},{"location":"server/ExecuteThreadRunner/#creating-instance","title":"Creating Instance","text":"<p><code>ExecuteThreadRunner</code> takes the following to be created:</p> <ul> <li> <code>ExecuteHolder</code>"},{"location":"server/ExpressionPlugin/","title":"ExpressionPlugin","text":"<p><code>ExpressionPlugin</code> is...FIXME</p>"},{"location":"server/RelationPlugin/","title":"RelationPlugin","text":"<p><code>RelationPlugin</code> is an abstraction of extensions that can transform.</p>"},{"location":"server/RelationPlugin/#contract","title":"Contract","text":""},{"location":"server/RelationPlugin/#transform","title":"Transform Relation","text":"<pre><code>transform(\n  relation: protobuf.Any,\n  planner: SparkConnectPlanner): Option[LogicalPlan]\n</code></pre> <p>Used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to handle plugins for Spark Connect relation types</li> </ul>"},{"location":"server/RelationPlugin/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"server/SessionHolder/","title":"SessionHolder","text":"<p><code>SessionHolder</code> is...FIXME</p>"},{"location":"server/SimpleSparkConnectService/","title":"SimpleSparkConnectService","text":"<p><code>SimpleSparkConnectService</code> is...FIXME</p>"},{"location":"server/SparkConnectAddArtifactsHandler/","title":"SparkConnectAddArtifactsHandler","text":"<p><code>SparkConnectAddArtifactsHandler</code> is...FIXME</p>"},{"location":"server/SparkConnectAnalyzeHandler/","title":"SparkConnectAnalyzeHandler","text":""},{"location":"server/SparkConnectAnalyzeHandler/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectAnalyzeHandler</code> takes the following to be created:</p> <ul> <li> <code>StreamObserver</code> of <code>AnalyzePlanResponse</code>s <p><code>SparkConnectAnalyzeHandler</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to handle an analyze plan request</li> </ul>"},{"location":"server/SparkConnectExecutePlanHandler/","title":"SparkConnectExecutePlanHandler","text":"<p><code>SparkConnectExecutePlanHandler</code> is...FIXME</p>"},{"location":"server/SparkConnectExecutionManager/","title":"SparkConnectExecutionManager","text":""},{"location":"server/SparkConnectExecutionManager/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectExecutionManager</code> takes no arguments to be created.</p> <p><code>SparkConnectExecutionManager</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested for the executionManager</li> </ul>"},{"location":"server/SparkConnectInterceptorRegistry/","title":"SparkConnectInterceptorRegistry","text":"<p><code>SparkConnectInterceptorRegistry</code> is...FIXME</p>"},{"location":"server/SparkConnectPlanExecution/","title":"SparkConnectPlanExecution","text":"<p><code>SparkConnectPlanExecution</code> is...FIXME</p>"},{"location":"server/SparkConnectPlanner/","title":"SparkConnectPlanner","text":""},{"location":"server/SparkConnectPlanner/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectPlanner</code> takes the following to be created:</p> <ul> <li> SessionHolder <p><code>SparkConnectPlanner</code> is created when:</p> <ul> <li><code>ExecuteThreadRunner</code> is requested to handle a command</li> <li><code>SparkConnectPlanExecution</code> is requested to handle a plan</li> <li><code>SparkConnectAnalyzeHandler</code> is requested to process an analyze plan request</li> </ul>"},{"location":"server/SparkConnectPlanner/#transformRelation","title":"transformRelation","text":"<pre><code>transformRelation(\n  rel: proto.Relation): LogicalPlan\n</code></pre> <p><code>transformRelation</code>...FIXME</p> <p><code>transformRelation</code> is used when:</p> <ul> <li><code>SparkConnectPlanExecution</code> is requested to handlePlan</li> <li><code>SparkConnectPlanner</code> is requested to do many things</li> <li><code>SparkConnectAnalyzeHandler</code> is requested to process a request</li> </ul>"},{"location":"server/SparkConnectPlanner/#transformRelationPlugin","title":"Handle Plugins for Spark Connect Relation Types","text":"<pre><code>transformRelationPlugin(\n  extension: ProtoAny): LogicalPlan\n</code></pre> <p><code>transformRelationPlugin</code>...FIXME</p>"},{"location":"server/SparkConnectPlanner/#transformMapPartitions","title":"transformMapPartitions","text":"<pre><code>transformMapPartitions(\n  rel: proto.MapPartitions): LogicalPlan\n</code></pre> <p><code>transformMapPartitions</code>...FIXME</p>"},{"location":"server/SparkConnectPlanner/#transformPythonUDF","title":"transformPythonUDF","text":"<pre><code>transformPythonUDF(\n  fun: proto.CommonInlineUserDefinedFunction): PythonUDF\n</code></pre> <p><code>transformPythonUDF</code> creates a <code>PythonUDF</code> (based on the given <code>fun</code> and transformPythonFunction).</p> <p><code>transformPythonUDF</code> is used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformMapPartitions, transformGroupMap, transformCoGroupMap, transformCommonInlineUserDefinedFunction</li> </ul>"},{"location":"server/SparkConnectPlanner/#transformPythonFunction","title":"transformPythonFunction","text":"<pre><code>transformPythonFunction(\n  fun: proto.PythonUDF): SimplePythonFunction\n</code></pre> <p><code>transformPythonFunction</code>...FIXME</p> <p><code>transformPythonFunction</code> is used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformPythonUDF and handleRegisterPythonUDF</li> </ul>"},{"location":"server/SparkConnectPlanner/#process","title":"Process Command","text":"<pre><code>process(\n  command: proto.Command,\n  responseObserver: StreamObserver[ExecutePlanResponse],\n  executeHolder: ExecuteHolder): Unit\n</code></pre> <p><code>process</code> handles the input <code>Command</code> based on its type.</p> Command Type Handler <code>REGISTER_FUNCTION</code> handleRegisterUserDefinedFunction <code>REGISTER_TABLE_FUNCTION</code> handleRegisterUserDefinedTableFunction <code>WRITE_OPERATION</code> handleWriteOperation <code>CREATE_DATAFRAME_VIEW</code> handleCreateViewCommand <code>WRITE_OPERATION_V2</code> handleWriteOperationV2 <code>EXTENSION</code> handleCommandPlugin <code>SQL_COMMAND</code> handleSqlCommand <code>WRITE_STREAM_OPERATION_START</code> handleWriteStreamOperationStart <code>STREAMING_QUERY_COMMAND</code> handleStreamingQueryCommand <code>STREAMING_QUERY_MANAGER_COMMAND</code> handleStreamingQueryManagerCommand <code>GET_RESOURCES_COMMAND</code> handleGetResourcesCommand <p><code>process</code> is used when:</p> <ul> <li><code>ExecuteThreadRunner</code> is requested to handle a command</li> </ul>"},{"location":"server/SparkConnectPlanner/#handleCommandPlugin","title":"handleCommandPlugin","text":"<pre><code>handleCommandPlugin(\n  extension: ProtoAny,\n  executeHolder: ExecuteHolder): Unit\n</code></pre> <p><code>handleCommandPlugin</code>...FIXME</p>"},{"location":"server/SparkConnectPlugin/","title":"SparkConnectPlugin","text":"<p><code>SparkConnectPlugin</code> is a Spark driver plugin (a <code>SparkPlugin</code> (Apache Spark) with the driver-side component only).</p> <p><code>SparkConnectPlugin</code> is the main entry point for Spark Connect in Apache Spark applications.</p>"},{"location":"server/SparkConnectPlugin/#driverPlugin","title":"Driver-Side Component","text":"SparkPlugin <pre><code>driverPlugin(): DriverPlugin\n</code></pre> <p><code>driverPlugin</code> is part of the <code>SparkPlugin</code> (Apache Spark) abstraction.</p> <p><code>driverPlugin</code> creates a new <code>DriverPlugin</code> (Apache Spark) that does the following:</p> <ul> <li>Starts a SparkConnectService when requested to initialize</li> <li>Stops the SparkConnectService when requested to shut down</li> </ul>"},{"location":"server/SparkConnectReleaseSessionHandler/","title":"SparkConnectReleaseSessionHandler","text":"<p><code>SparkConnectReleaseSessionHandler</code> is...FIXME</p>"},{"location":"server/SparkConnectServer/","title":"SparkConnectServer Standalone Application","text":"<p><code>SparkConnectServer</code> is a standalone application to start a Spark Connect server from command line.</p> <p><code>SparkConnectServer</code> can be started using <code>sbin/start-connect-server.sh</code> shell script.</p> <p><code>SparkConnectServer</code> can be stopped using <code>sbin/stop-connect-server.sh</code> shell script.</p>"},{"location":"server/SparkConnectServer/#main","title":"Launching Application","text":"<pre><code>main(\n  args: Array[String]): Unit\n</code></pre> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Starting Spark session.\n</code></pre> <p><code>main</code> creates a <code>SparkSession</code>.</p> <p><code>main</code> starts a SparkConnectService.</p> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Spark Connect server started.\n</code></pre> <p>In the end, <code>main</code> is paused until the SparkConnectService is terminated.</p>"},{"location":"server/SparkConnectServerListener/","title":"SparkConnectServerListener","text":"<p><code>SparkConnectServerListener</code> is a <code>SparkListener</code> (Apache Spark).</p>"},{"location":"server/SparkConnectServerTab/","title":"SparkConnectServerTab","text":"<p><code>SparkConnectServerTab</code> is...FIXME</p>"},{"location":"server/SparkConnectService/","title":"SparkConnectService","text":"<p><code>SparkConnectService</code> is a <code>BindableService</code> (gRPC).</p> <p><code>SparkConnectService</code> is started as a gRPC service for the following:</p> <ul> <li>Apache Spark applications (as a Spark driver plugin)</li> <li>On command line as a SparkConnectServer standalone application</li> </ul> <p>Right after <code>SparkConnectService</code> has been started, a <code>SparkListenerConnectServiceStarted</code> event is posted with all the network connectivity information.</p>"},{"location":"server/SparkConnectService/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectService</code> takes the following to be created:</p> <ul> <li> <code>debug</code> flag <p><code>SparkConnectService</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to start a gRPC service</li> </ul>"},{"location":"server/SparkConnectService/#server","title":"gRPC Server","text":"<p><code>SparkConnectService</code> creates and starts a <code>Server</code> (gRPC) when starting the gRPC Service.</p>"},{"location":"server/SparkConnectService/#start","title":"Start Spark Connect Service","text":"<pre><code>start(\n  sc: SparkContext): Unit\n</code></pre> <p><code>start</code> starts the gRPC service and then creates a listener and the UI.</p> <p>In the end, <code>start</code> posts a SparkListenerConnectServiceStarted event.</p> <p><code>start</code> is used when:</p> <ul> <li><code>SparkConnectPlugin</code> is requested for the Spark driver plugin</li> <li>SparkConnectServer standalone application is started</li> </ul>"},{"location":"server/SparkConnectService/#startGRPCService","title":"Start gRPC Service","text":"<pre><code>startGRPCService(): Unit\n</code></pre> <p><code>startGRPCService</code> reads the values of the following configuration properties:</p> Configuration Property Default Value spark.connect.grpc.debug.enabled <code>true</code> spark.connect.grpc.binding.port <code>15002</code> spark.connect.grpc.maxInboundMessageSize <code>128 * 1024 * 1024</code> <p><code>startGRPCService</code> builds a <code>NettyServerBuilder</code> with the <code>spark.connect.grpc.binding.port</code> and a SparkConnectService.</p> <p><code>startGRPCService</code> registers interceptors.</p> <p><code>startGRPCService</code> builds the server and starts it.</p> <p>If successful, prints out the following INFO message to the logs:</p> <pre><code>Successfully started service [serviceName] on port [port].\n</code></pre>"},{"location":"server/SparkConnectService/#createListenerAndUI","title":"createListenerAndUI","text":"<pre><code>createListenerAndUI(\n  sc: SparkContext): Unit\n</code></pre> <p><code>createListenerAndUI</code> creates a SparkConnectServerTab (for <code>spark.ui.enabled</code> enabled).</p>"},{"location":"server/SparkConnectService/#postSparkConnectServiceStarted","title":"Post SparkListenerConnectServiceStarted","text":"<pre><code>postSparkConnectServiceStarted(): Unit\n</code></pre> <p><code>postSparkConnectServiceStarted</code> posts a <code>SparkListenerConnectServiceStarted</code> event (with this server's hostAddress, port and the current time)</p>"},{"location":"server/SparkConnectService/#addArtifacts","title":"Handle Add Artifacts Request","text":"Generated by gRPC Proto Compiler <pre><code>addArtifacts(\n  responseObserver: StreamObserver[AddArtifactsResponse]\n): StreamObserver[AddArtifactsRequest]\n</code></pre> <p><code>addArtifacts</code> is generated by the gRPC proto compiler from <code>spark/connect/base.proto</code>.</p> <p><code>addArtifacts</code> creates a new SparkConnectAddArtifactsHandler for the given <code>responseObserver</code>.</p>"},{"location":"server/SparkConnectService/#analyzePlan","title":"Handle Analyze Plan Request","text":"Generated by gRPC Proto Compiler <pre><code>analyzePlan(\n  request: proto.AnalyzePlanRequest,\n  responseObserver: StreamObserver[proto.AnalyzePlanResponse]): Unit\n</code></pre> <p><code>analyzePlan</code> is generated by the gRPC proto compiler from <code>spark/connect/base.proto</code>.</p> <p><code>analyzePlan</code> creates a new SparkConnectAnalyzeHandler to handle the <code>AnalyzePlanRequest</code> request.</p>"},{"location":"server/SparkConnectService/#executePlan","title":"Handle Execute Plan Request","text":"Generated by gRPC Proto Compiler <pre><code>executePlan(\n  request: proto.ExecutePlanRequest,\n  responseObserver: StreamObserver[proto.ExecutePlanResponse]): Unit\n</code></pre> <p><code>executePlan</code> is generated by the gRPC proto compiler from <code>spark/connect/base.proto</code>.</p> <p><code>executePlan</code> creates a SparkConnectExecutePlanHandler to handle the <code>ExecutePlanRequest</code> request.</p>"},{"location":"server/SparkConnectService/#releaseSession","title":"Handle Execute Plan Request","text":"Generated by gRPC Proto Compiler <pre><code>releaseSession(\n  request: proto.ReleaseSessionRequest,\n  responseObserver: StreamObserver[proto.ReleaseSessionResponse]): Unit\n</code></pre> <p><code>releaseSession</code> is generated by the gRPC proto compiler from <code>spark/connect/base.proto</code>.</p> <p><code>releaseSession</code> creates a SparkConnectReleaseSessionHandler to handle the <code>ReleaseSessionRequest</code> request.</p>"},{"location":"server/SparkConnectSessionManager/","title":"SparkConnectSessionManager","text":"<p><code>SparkConnectSessionManager</code> is...FIXME</p>"},{"location":"server/configuration-properties/","title":"Server's Configuration Properties","text":""},{"location":"server/configuration-properties/#sparkconnectgrpc","title":"spark.connect.grpc","text":""},{"location":"server/configuration-properties/#spark.connect.grpc.binding.address","title":"binding.address <p>spark.connect.grpc.binding.address</p> <p>Default: (undefined)</p>","text":""},{"location":"server/configuration-properties/#spark.connect.grpc.binding.port","title":"binding.port <p>spark.connect.grpc.binding.port</p> <p>Default: <code>15002</code></p>","text":""},{"location":"server/configuration-properties/#spark.connect.grpc.debug.enabled","title":"debug.enabled <p>spark.connect.grpc.debug.enabled</p> <p>Default: <code>true</code></p>","text":""},{"location":"server/configuration-properties/#spark.connect.grpc.interceptor.classes","title":"interceptor.classes <p>spark.connect.grpc.interceptor.classes</p> <p>A comma-separated list of class names that must implement the <code>io.grpc.ServerInterceptor</code> interface.</p> <p>Default: (undefined)</p>","text":""},{"location":"server/configuration-properties/#spark.connect.grpc.port.maxRetries","title":"port.maxRetries <p>spark.connect.grpc.port.maxRetries</p> <p>The maximum number of port retry attempts for the gRPC server binding.</p> <p>Default: <code>0</code></p>","text":""},{"location":"sql/","title":"Spark SQL","text":"<p>Note</p> <p>Learn more in The Internals of Spark SQL.</p>"},{"location":"sql/SQLExecution/","title":"SQLExecution","text":""},{"location":"sql/SQLExecution/#executionIdJobTag","title":"executionIdJobTag","text":"<pre><code>executionIdJobTag(\n  session: SparkSession,\n  id: Long)\n</code></pre> <p><code>executionIdJobTag</code> is the following text (with the sessionJobTag of the given <code>SparkSession</code>):</p> <pre><code>[sessionJobTag]-execution-root-id-[id]\n</code></pre> <p><code>executionIdJobTag</code> is used when:</p> <ul> <li><code>SparkSession</code> is requested to interruptOperation</li> <li><code>SQLExecution</code> is requested to withNewExecutionId0</li> </ul>"},{"location":"sql/SQLExecution/#withNewExecutionId","title":"withNewExecutionId","text":"<pre><code>withNewExecutionId[T](\n  queryExecution: QueryExecution,\n  name: Option[String] = None)(\n  body: =&gt; T): T\n</code></pre> <p><code>withNewExecutionId</code> withNewExecutionId0 with the given <code>QueryExecution</code>, the optional <code>name</code> and the <code>body</code> (as a <code>Right</code> value).</p>"},{"location":"sql/SQLExecution/#withNewExecutionIdOnError","title":"withNewExecutionIdOnError","text":"<pre><code>withNewExecutionIdOnError(\n  queryExecution: QueryExecution,\n  name: Option[String] = None)(\n  t: Throwable): Unit\n</code></pre> <p><code>withNewExecutionIdOnError</code> withNewExecutionId0 with the given <code>QueryExecution</code>, the optional <code>name</code> and the <code>Throwable</code> (as a <code>Left</code> value).</p> <p><code>withNewExecutionIdOnError</code> is used when:</p> <ul> <li><code>QueryExecution</code> is requested to <code>assertAnalyzed</code></li> </ul>"},{"location":"sql/SQLExecution/#withExecutionId","title":"withExecutionId","text":"<pre><code>withExecutionId[T](\n  sparkSession: SparkSession,\n  executionId: String)(\n  body: =&gt; T): T\n</code></pre> <p><code>withExecutionId</code>...FIXME</p> <p><code>withExecutionId</code> is used when:</p> <ul> <li><code>SubqueryExec</code> physical operator is requested for the <code>relationFuture</code></li> <li><code>SubqueryBroadcastExec</code> physical operator is requested for the <code>relationFuture</code></li> </ul>"},{"location":"sql/SQLExecution/#withThreadLocalCaptured","title":"withThreadLocalCaptured","text":"<pre><code>withThreadLocalCaptured[T](\n  sparkSession: SparkSession,\n  exec: ExecutorService)(\n  body: =&gt; T): Future[T]\n</code></pre> <p><code>withThreadLocalCaptured</code>...FIXME</p> <p><code>withThreadLocalCaptured</code> is used when:</p> <ul> <li><code>BroadcastExchangeExec</code> physical operator is requested for the <code>relationFuture</code></li> <li><code>BroadcastExchangeLike</code> physical operator is requested for the <code>triggerFuture</code></li> <li><code>SubqueryExec</code> physical operator is requested for the <code>relationFuture</code></li> <li><code>SubqueryBroadcastExec</code> physical operator is requested for the <code>relationFuture</code></li> <li><code>ShuffleExchangeLike</code> physical operator is requested for the <code>triggerFuture</code></li> </ul>"},{"location":"sql/SQLExecution/#withNewExecutionId0","title":"withNewExecutionId0","text":"<pre><code>withNewExecutionId0[T](\n  queryExecution: QueryExecution,\n  name: Option[String] = None)(\n  body: Either[Throwable, () =&gt; T]): T\n</code></pre> <p><code>withNewExecutionId0</code>...FIXME</p> <p><code>withNewExecutionId0</code> is used when:</p> <ul> <li><code>SQLExecution</code> is requested to withNewExecutionId and withNewExecutionIdOnError</li> </ul>"},{"location":"sql/SQLExecution/#withSessionTagsApplied","title":"withSessionTagsApplied","text":"<pre><code>withSessionTagsApplied[T](\n  sparkSession: SparkSession)(\n  block: =&gt; T): T\n</code></pre> <p><code>withSessionTagsApplied</code>...FIXME</p> <p><code>withSessionTagsApplied</code> is used when:</p> <ul> <li><code>SQLExecution</code> is requested to withExecutionId, withNewExecutionId0, and withThreadLocalCaptured</li> </ul>"},{"location":"sql/SparkSession/","title":"SparkSession","text":"<p><code>SparkSession</code> is the entry point to Spark SQL.</p>"},{"location":"sql/SparkSession/#sessionUUID","title":"Session UUID","text":"<pre><code>sessionUUID: String\n</code></pre> <p><code>SparkSession</code> generates a random UUID when created.</p> <p><code>sessionUUID</code> is used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformCachedLocalRelation</li> <li><code>SessionHolder</code> is requested to <code>serverSessionId</code></li> <li><code>SparkConnectArtifactStatusesHandler</code> is requested to <code>cacheExists</code></li> <li><code>SparkConnectSessionManager</code> is requested to getIsolatedSession, getOrCreateIsolatedSession</li> <li><code>SparkSession</code> is requested to addTag, sessionJobTag</li> <li><code>ArtifactManager</code> is requested to many things</li> <li><code>ExecutionListenerBus</code> is created (one per <code>SparkSession</code>) and requested to <code>doPostEvent</code></li> </ul>"},{"location":"sql/SparkSession/#sessionJobTag","title":"sessionJobTag","text":"<pre><code>sessionJobTag: String\n</code></pre> <p><code>sessionJobTag</code> is the following text (with this sessionUUID):</p> <pre><code>spark-session-$sessionUUID\n</code></pre> <p><code>sessionJobTag</code> is used when:</p> <ul> <li><code>SparkSession</code> is requested to interruptAll</li> <li><code>SQLExecution</code> is requested for the executionIdJobTag and to withSessionTagsApplied</li> </ul>"},{"location":"sql/SparkSession/#interruptOperation","title":"interruptOperation","text":"<pre><code>interruptOperation(\n  operationId: String): Seq[String]\n</code></pre> <p>Public API</p> <p><code>interruptAll</code> is part of the public API.</p> <p><code>interruptOperation</code>...FIXME</p>"},{"location":"sql/SparkSession/#interruptAll","title":"interruptAll","text":"<pre><code>interruptAll(): Seq[String]\n</code></pre> <p>Public API</p> <p><code>interruptAll</code> is part of the public API.</p> <p><code>interruptAll</code> doInterruptTag with this sessionJobTag and the following reason:</p> <pre><code>as part of cancellation of all jobs\n</code></pre>"}]}