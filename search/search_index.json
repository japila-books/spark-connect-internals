{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of Spark Connect (Apache Spark 4.0.0-SNAPSHOT)","text":"<p>Welcome to The Internals of Spark Connect online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, a Freelance Data(bricks) Engineer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams) with brief forays into a wider data engineering space (mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Connect as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Spark Connect \ud83d\udd25</p> <p>Last update: 2024-12-15</p>"},{"location":"logging/","title":"Logging","text":"<p>Spark Connect uses the same logging infrastructure as Apache Spark.</p>"},{"location":"overview/","title":"Spark Connect","text":"<p>Spark Connect is a client-server interface for Apache Spark for remote connectivity to Spark clusters (using the DataFrame API and unresolved logical plans as the protocol based on gRPC Java).</p> Apache Spark 3.4 <p>Spark Connect has been available since Apache Spark 3.4.</p> <p>The Spark Connect server can be started using <code>sbin/start-connect-server.sh</code> shell script.</p> <pre><code>$ ./sbin/start-connect-server.sh\nstarting org.apache.spark.sql.connect.service.SparkConnectServer, logging to...\n\n$ tail -1 logs/spark-jacek-org.apache.spark.sql.connect.service.SparkConnectServer-1-Jaceks-Mac-mini.local.out\n... Spark Connect server started.\n</code></pre> <p>Use Spark Connect for interactive analysis:</p> <pre><code>./bin/pyspark --remote \"sc://localhost\"\n</code></pre> <p>And you will notice that the PySpark shell welcome message tells you that you have connected to Spark using Spark Connect:</p> <pre><code>Client connected to the Spark Connect server at localhost\n</code></pre> <p>Check the Spark session type:</p> <pre><code>SparkSession available as 'spark'.\n&gt;&gt;&gt; type(spark)\n&lt;class 'pyspark.sql.connect.session.SparkSession'&gt;\n</code></pre> <p>Now you can run PySpark code in the shell to see Spark Connect in action:</p> <pre><code>&gt;&gt;&gt; columns = [\"id\",\"name\"]\n&gt;&gt;&gt; data = [(1,\"Sarah\"),(2,\"Maria\")]\n&gt;&gt;&gt; df = spark.createDataFrame(data).toDF(*columns)\n&gt;&gt;&gt; df.show()\n+---+-----+\n| id| name|\n+---+-----+\n|  1|Sarah|\n|  2|Maria|\n+---+-----+\n</code></pre>"},{"location":"client/","title":"Spark Connect Client","text":"<p>Spark Connect Client can be started using...FIXME</p>"},{"location":"server/","title":"Spark Connect Server","text":"<p>Spark Connect Server is a gRPC service that can be started for the following environments:</p> <ul> <li>Apache Spark applications (as a Spark driver plugin)</li> <li>On command line using sbin/start-connect-server.sh utility</li> </ul>"},{"location":"server/CommandPlugin/","title":"CommandPlugin","text":"<p><code>CommandPlugin</code> is an abstraction of command processors in Spark Connect that can process a command.</p>"},{"location":"server/CommandPlugin/#contract","title":"Contract","text":""},{"location":"server/CommandPlugin/#process","title":"Process Command","text":"<pre><code>process(\n  command: protobuf.Any,\n  planner: SparkConnectPlanner): Option[Unit]\n</code></pre> <p>Used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to handleCommandPlugin</li> </ul>"},{"location":"server/CommandPlugin/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"server/ExecuteHolder/","title":"ExecuteHolder","text":""},{"location":"server/ExecuteHolder/#creating-instance","title":"Creating Instance","text":"<p><code>ExecuteHolder</code> takes the following to be created:</p> <ul> <li> <code>ExecutePlanRequest</code> <li> <code>SessionHolder</code> <p><code>ExecuteHolder</code> is created when:</p> <ul> <li><code>SparkConnectExecutionManager</code> is requested to create an ExecuteHolder</li> </ul>"},{"location":"server/ExecuteThreadRunner/","title":"ExecuteThreadRunner","text":"<p><code>ExecuteThreadRunner</code> is created alongside ExecuteHolder.</p>"},{"location":"server/ExecuteThreadRunner/#creating-instance","title":"Creating Instance","text":"<p><code>ExecuteThreadRunner</code> takes the following to be created:</p> <ul> <li> <code>ExecuteHolder</code>"},{"location":"server/ExpressionPlugin/","title":"ExpressionPlugin","text":"<p><code>ExpressionPlugin</code> is...FIXME</p>"},{"location":"server/RelationPlugin/","title":"RelationPlugin","text":"<p><code>RelationPlugin</code> is an abstraction of extensions that can transform.</p>"},{"location":"server/RelationPlugin/#contract","title":"Contract","text":""},{"location":"server/RelationPlugin/#transform","title":"Transform Relation","text":"<pre><code>transform(\n  relation: protobuf.Any,\n  planner: SparkConnectPlanner): Option[LogicalPlan]\n</code></pre> <p>Used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to handle plugins for Spark Connect relation types</li> </ul>"},{"location":"server/RelationPlugin/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"server/SessionHolder/","title":"SessionHolder","text":"<p><code>SessionHolder</code> is...FIXME</p>"},{"location":"server/SimpleSparkConnectService/","title":"SimpleSparkConnectService","text":"<p><code>SimpleSparkConnectService</code> is...FIXME</p>"},{"location":"server/SparkConnectAnalyzeHandler/","title":"SparkConnectAnalyzeHandler","text":""},{"location":"server/SparkConnectAnalyzeHandler/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectAnalyzeHandler</code> takes the following to be created:</p> <ul> <li> <code>StreamObserver</code> of <code>AnalyzePlanResponse</code>s <p><code>SparkConnectAnalyzeHandler</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to handle an analyze plan request</li> </ul>"},{"location":"server/SparkConnectExecutePlanHandler/","title":"SparkConnectExecutePlanHandler","text":"<p><code>SparkConnectExecutePlanHandler</code> is...FIXME</p>"},{"location":"server/SparkConnectExecutionManager/","title":"SparkConnectExecutionManager","text":""},{"location":"server/SparkConnectExecutionManager/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectExecutionManager</code> takes no arguments to be created.</p> <p><code>SparkConnectExecutionManager</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested for the executionManager</li> </ul>"},{"location":"server/SparkConnectInterceptorRegistry/","title":"SparkConnectInterceptorRegistry","text":"<p><code>SparkConnectInterceptorRegistry</code> is...FIXME</p>"},{"location":"server/SparkConnectPlanExecution/","title":"SparkConnectPlanExecution","text":"<p><code>SparkConnectPlanExecution</code> is...FIXME</p>"},{"location":"server/SparkConnectPlanner/","title":"SparkConnectPlanner","text":""},{"location":"server/SparkConnectPlanner/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectPlanner</code> takes the following to be created:</p> <ul> <li> SessionHolder <p><code>SparkConnectPlanner</code> is created when:</p> <ul> <li><code>ExecuteThreadRunner</code> is requested to handle a command</li> <li><code>SparkConnectPlanExecution</code> is requested to handle a plan</li> <li><code>SparkConnectAnalyzeHandler</code> is requested to process an analyze plan request</li> </ul>"},{"location":"server/SparkConnectPlanner/#transformRelation","title":"transformRelation","text":"<pre><code>transformRelation(\n  rel: proto.Relation): LogicalPlan\n</code></pre> <p><code>transformRelation</code>...FIXME</p> <p><code>transformRelation</code> is used when:</p> <ul> <li><code>SparkConnectPlanExecution</code> is requested to handlePlan</li> <li><code>SparkConnectPlanner</code> is requested to do many things</li> <li><code>SparkConnectAnalyzeHandler</code> is requested to process a request</li> </ul>"},{"location":"server/SparkConnectPlanner/#transformRelationPlugin","title":"Handle Plugins for Spark Connect Relation Types","text":"<pre><code>transformRelationPlugin(\n  extension: ProtoAny): LogicalPlan\n</code></pre> <p><code>transformRelationPlugin</code>...FIXME</p>"},{"location":"server/SparkConnectPlanner/#transformMapPartitions","title":"transformMapPartitions","text":"<pre><code>transformMapPartitions(\n  rel: proto.MapPartitions): LogicalPlan\n</code></pre> <p><code>transformMapPartitions</code>...FIXME</p>"},{"location":"server/SparkConnectPlanner/#transformPythonUDF","title":"transformPythonUDF","text":"<pre><code>transformPythonUDF(\n  fun: proto.CommonInlineUserDefinedFunction): PythonUDF\n</code></pre> <p><code>transformPythonUDF</code> creates a <code>PythonUDF</code> (based on the given <code>fun</code> and transformPythonFunction).</p> <p><code>transformPythonUDF</code> is used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformMapPartitions, transformGroupMap, transformCoGroupMap, transformCommonInlineUserDefinedFunction</li> </ul>"},{"location":"server/SparkConnectPlanner/#transformPythonFunction","title":"transformPythonFunction","text":"<pre><code>transformPythonFunction(\n  fun: proto.PythonUDF): SimplePythonFunction\n</code></pre> <p><code>transformPythonFunction</code>...FIXME</p> <p><code>transformPythonFunction</code> is used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformPythonUDF and handleRegisterPythonUDF</li> </ul>"},{"location":"server/SparkConnectPlanner/#process","title":"Process Command","text":"<pre><code>process(\n  command: proto.Command,\n  responseObserver: StreamObserver[ExecutePlanResponse],\n  executeHolder: ExecuteHolder): Unit\n</code></pre> <p><code>process</code> handles the input <code>Command</code> based on its type.</p> Command Type Handler <code>REGISTER_FUNCTION</code> handleRegisterUserDefinedFunction <code>REGISTER_TABLE_FUNCTION</code> handleRegisterUserDefinedTableFunction <code>WRITE_OPERATION</code> handleWriteOperation <code>CREATE_DATAFRAME_VIEW</code> handleCreateViewCommand <code>WRITE_OPERATION_V2</code> handleWriteOperationV2 <code>EXTENSION</code> handleCommandPlugin <code>SQL_COMMAND</code> handleSqlCommand <code>WRITE_STREAM_OPERATION_START</code> handleWriteStreamOperationStart <code>STREAMING_QUERY_COMMAND</code> handleStreamingQueryCommand <code>STREAMING_QUERY_MANAGER_COMMAND</code> handleStreamingQueryManagerCommand <code>GET_RESOURCES_COMMAND</code> handleGetResourcesCommand <p><code>process</code> is used when:</p> <ul> <li><code>ExecuteThreadRunner</code> is requested to handle a command</li> </ul>"},{"location":"server/SparkConnectPlanner/#handleCommandPlugin","title":"handleCommandPlugin","text":"<pre><code>handleCommandPlugin(\n  extension: ProtoAny,\n  executeHolder: ExecuteHolder): Unit\n</code></pre> <p><code>handleCommandPlugin</code>...FIXME</p>"},{"location":"server/SparkConnectPlugin/","title":"SparkConnectPlugin","text":"<p><code>SparkConnectPlugin</code> is a Spark driver plugin (a <code>SparkPlugin</code> (Apache Spark) with the driver-side component only).</p> <p><code>SparkConnectPlugin</code> is the main entry point for Spark Connect in Apache Spark applications.</p>"},{"location":"server/SparkConnectPlugin/#driverPlugin","title":"Driver-Side Component","text":"SparkPlugin <pre><code>driverPlugin(): DriverPlugin\n</code></pre> <p><code>driverPlugin</code> is part of the <code>SparkPlugin</code> (Apache Spark) abstraction.</p> <p><code>driverPlugin</code> creates a new <code>DriverPlugin</code> (Apache Spark) that does the following:</p> <ul> <li>Starts a SparkConnectService when requested to initialize</li> <li>Stops the SparkConnectService when requested to shut down</li> </ul>"},{"location":"server/SparkConnectServer/","title":"SparkConnectServer Standalone Application","text":"<p><code>SparkConnectServer</code> is a standalone application to start a Spark Connect server from command line.</p> <p><code>SparkConnectServer</code> can be started using <code>sbin/start-connect-server.sh</code> shell script.</p> <p><code>SparkConnectServer</code> can be stopped using <code>sbin/stop-connect-server.sh</code> shell script.</p>"},{"location":"server/SparkConnectServer/#main","title":"Launching Application","text":"<pre><code>main(\n  args: Array[String]): Unit\n</code></pre> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Starting Spark session.\n</code></pre> <p><code>main</code> creates a <code>SparkSession</code>.</p> <p><code>main</code> starts a SparkConnectService.</p> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Spark Connect server started.\n</code></pre> <p>In the end, <code>main</code> is paused until the SparkConnectService is terminated.</p>"},{"location":"server/SparkConnectServerListener/","title":"SparkConnectServerListener","text":"<p><code>SparkConnectServerListener</code> is a <code>SparkListener</code> (Apache Spark).</p>"},{"location":"server/SparkConnectServerTab/","title":"SparkConnectServerTab","text":"<p><code>SparkConnectServerTab</code> is...FIXME</p>"},{"location":"server/SparkConnectService/","title":"SparkConnectService","text":"<p><code>SparkConnectService</code> is a <code>BindableService</code> (gRPC).</p> <p><code>SparkConnectService</code> is started as a gRPC service for the following:</p> <ul> <li>Apache Spark applications (as a Spark driver plugin)</li> <li>On command line as a SparkConnectServer standalone application</li> </ul>"},{"location":"server/SparkConnectService/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectService</code> takes the following to be created:</p> <ul> <li> <code>debug</code> flag <p><code>SparkConnectService</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to start a gRPC service</li> </ul>"},{"location":"server/SparkConnectService/#server","title":"gRPC Server","text":"<p><code>SparkConnectService</code> creates and starts a <code>Server</code> (gRPC) when starting the gRPC Service.</p>"},{"location":"server/SparkConnectService/#start","title":"Start Spark Connect Service","text":"<pre><code>start(\n  sc: SparkContext): Unit\n</code></pre> <p><code>start</code> starts the gRPC service and then creates a listener and the UI.</p> <p><code>start</code> is used when:</p> <ul> <li><code>SparkConnectPlugin</code> is requested for the Spark driver plugin</li> <li>SparkConnectServer standalone application is started</li> </ul>"},{"location":"server/SparkConnectService/#startGRPCService","title":"Start gRPC Service","text":"<pre><code>startGRPCService(): Unit\n</code></pre> <p><code>startGRPCService</code> reads the values of the following configuration properties:</p> Configuration Property Default Value <code>spark.connect.grpc.debug.enabled</code> <code>true</code> <code>spark.connect.grpc.binding.port</code> <code>15002</code> <code>spark.connect.grpc.maxInboundMessageSize</code> <code>128 * 1024 * 1024</code> <p><code>startGRPCService</code> builds a <code>NettyServerBuilder</code> with the <code>spark.connect.grpc.binding.port</code> and a SparkConnectService.</p> <p><code>startGRPCService</code> registers interceptors.</p> <p><code>startGRPCService</code> builds the server and starts it.</p>"},{"location":"server/SparkConnectService/#createListenerAndUI","title":"createListenerAndUI","text":"<pre><code>createListenerAndUI(\n  sc: SparkContext): Unit\n</code></pre> <p><code>createListenerAndUI</code> creates a SparkConnectServerTab (for <code>spark.ui.enabled</code> enabled).</p>"},{"location":"server/SparkConnectService/#analyzePlan","title":"Handle Analyze Plan Request","text":"Generated by gRPC Proto Compiler <pre><code>analyzePlan(\n  request: proto.AnalyzePlanRequest,\n  responseObserver: StreamObserver[proto.AnalyzePlanResponse]): Unit\n</code></pre> <p><code>analyzePlan</code> is generated by the gRPC proto compiler from <code>spark/connect/base.proto</code>.</p> <p><code>analyzePlan</code> creates a new SparkConnectAnalyzeHandler to handle the <code>AnalyzePlanRequest</code> request.</p>"},{"location":"server/SparkConnectService/#executePlan","title":"Handle Execute Plan Request","text":"Generated by gRPC Proto Compiler <pre><code>executePlan(\n  request: proto.ExecutePlanRequest,\n  responseObserver: StreamObserver[proto.ExecutePlanResponse]): Unit\n</code></pre> <p><code>executePlan</code> is generated by the gRPC proto compiler from <code>spark/connect/base.proto</code>.</p> <p><code>executePlan</code> creates a SparkConnectExecutePlanHandler to handle the <code>ExecutePlanRequest</code> request.</p>"}]}