{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of Spark Connect (Apache Spark 3.5.3)","text":"<p>Welcome to The Internals of Spark Connect online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, a Freelance Data(bricks) Engineer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams) with brief forays into a wider data engineering space (mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Connect as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Spark Connect \ud83d\udd25</p> <p>Last update: 2024-11-30</p>"},{"location":"CommandPlugin/","title":"CommandPlugin","text":"<p><code>CommandPlugin</code> is an abstraction of command processors in Spark Connect that can process a command.</p>"},{"location":"CommandPlugin/#contract","title":"Contract","text":""},{"location":"CommandPlugin/#process","title":"Process Command","text":"<pre><code>process(\n  command: protobuf.Any,\n  planner: SparkConnectPlanner): Option[Unit]\n</code></pre> <p>Used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to handleCommandPlugin</li> </ul>"},{"location":"CommandPlugin/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"ExecuteHolder/","title":"ExecuteHolder","text":""},{"location":"ExecuteHolder/#creating-instance","title":"Creating Instance","text":"<p><code>ExecuteHolder</code> takes the following to be created:</p> <ul> <li> <code>ExecutePlanRequest</code> <li> <code>SessionHolder</code> <p><code>ExecuteHolder</code> is created when:</p> <ul> <li><code>SparkConnectExecutionManager</code> is requested to create an ExecuteHolder</li> </ul>"},{"location":"ExecuteThreadRunner/","title":"ExecuteThreadRunner","text":"<p><code>ExecuteThreadRunner</code> is created alongside ExecuteHolder.</p>"},{"location":"ExecuteThreadRunner/#creating-instance","title":"Creating Instance","text":"<p><code>ExecuteThreadRunner</code> takes the following to be created:</p> <ul> <li> <code>ExecuteHolder</code>"},{"location":"ExpressionPlugin/","title":"ExpressionPlugin","text":"<p><code>ExpressionPlugin</code> is...FIXME</p>"},{"location":"RelationPlugin/","title":"RelationPlugin","text":"<p><code>RelationPlugin</code> is an abstraction of extensions that can transform.</p>"},{"location":"RelationPlugin/#contract","title":"Contract","text":""},{"location":"RelationPlugin/#transform","title":"transform <pre><code>transform(\n  relation: protobuf.Any,\n  planner: SparkConnectPlanner): Option[LogicalPlan]\n</code></pre> <p>Used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformRelationPlugin</li> </ul>","text":""},{"location":"RelationPlugin/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"SimpleSparkConnectService/","title":"SimpleSparkConnectService","text":"<p><code>SimpleSparkConnectService</code> is...FIXME</p>"},{"location":"SparkConnectAnalyzeHandler/","title":"SparkConnectAnalyzeHandler","text":""},{"location":"SparkConnectAnalyzeHandler/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectAnalyzeHandler</code> takes the following to be created:</p> <ul> <li> <code>StreamObserver</code> of <code>AnalyzePlanResponse</code>s <p><code>SparkConnectAnalyzeHandler</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to handle analyzePlan request</li> </ul>"},{"location":"SparkConnectExecutionManager/","title":"SparkConnectExecutionManager","text":""},{"location":"SparkConnectExecutionManager/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectExecutionManager</code> takes no arguments to be created.</p> <p><code>SparkConnectExecutionManager</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested for the executionManager</li> </ul>"},{"location":"SparkConnectInterceptorRegistry/","title":"SparkConnectInterceptorRegistry","text":"<p><code>SparkConnectInterceptorRegistry</code> is...FIXME</p>"},{"location":"SparkConnectPlanner/","title":"SparkConnectPlanner","text":""},{"location":"SparkConnectPlanner/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectPlanner</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <p><code>SparkConnectPlanner</code> is created when:</p> <ul> <li><code>SparkConnectAnalyzeHandler</code> is requested to process a request</li> <li><code>SparkConnectStreamHandler</code> is requested to handlePlan and handleCommand</li> </ul>"},{"location":"SparkConnectPlanner/#transformRelation","title":"transformRelation","text":"<pre><code>transformRelation(\n  rel: proto.Relation): LogicalPlan\n</code></pre> <p><code>transformRelation</code>...FIXME</p> <p><code>transformRelation</code> is used when:</p> <ul> <li><code>SparkConnectAnalyzeHandler</code> is requested to process a request</li> <li><code>SparkConnectStreamHandler</code> is requested to handlePlan</li> </ul>"},{"location":"SparkConnectPlanner/#transformRelationPlugin","title":"transformRelationPlugin","text":"<pre><code>transformRelationPlugin(\n  extension: ProtoAny): LogicalPlan\n</code></pre> <p><code>transformRelationPlugin</code>...FIXME</p>"},{"location":"SparkConnectPlanner/#transformMapPartitions","title":"transformMapPartitions","text":"<pre><code>transformMapPartitions(\n  rel: proto.MapPartitions): LogicalPlan\n</code></pre> <p><code>transformMapPartitions</code>...FIXME</p>"},{"location":"SparkConnectPlanner/#transformPythonUDF","title":"transformPythonUDF","text":"<pre><code>transformPythonUDF(\n  fun: proto.CommonInlineUserDefinedFunction): PythonUDF\n</code></pre> <p><code>transformPythonUDF</code> creates a <code>PythonUDF</code> (based on the given <code>fun</code> and transformPythonFunction).</p> <p><code>transformPythonUDF</code> is used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformMapPartitions, transformGroupMap, transformCoGroupMap, transformCommonInlineUserDefinedFunction</li> </ul>"},{"location":"SparkConnectPlanner/#transformPythonFunction","title":"transformPythonFunction","text":"<pre><code>transformPythonFunction(\n  fun: proto.PythonUDF): SimplePythonFunction\n</code></pre> <p><code>transformPythonFunction</code>...FIXME</p> <p><code>transformPythonFunction</code> is used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformPythonUDF and handleRegisterPythonUDF</li> </ul>"},{"location":"SparkConnectPlanner/#process","title":"Process Command","text":"<pre><code>process(\n  command: proto.Command,\n  responseObserver: StreamObserver[ExecutePlanResponse],\n  executeHolder: ExecuteHolder): Unit\n</code></pre> <p><code>process</code> handles the input <code>Command</code> based on its type.</p> Command Type Handler <code>REGISTER_FUNCTION</code> handleRegisterUserDefinedFunction <code>REGISTER_TABLE_FUNCTION</code> handleRegisterUserDefinedTableFunction <code>WRITE_OPERATION</code> handleWriteOperation <code>CREATE_DATAFRAME_VIEW</code> handleCreateViewCommand <code>WRITE_OPERATION_V2</code> handleWriteOperationV2 <code>EXTENSION</code> handleCommandPlugin <code>SQL_COMMAND</code> handleSqlCommand <code>WRITE_STREAM_OPERATION_START</code> handleWriteStreamOperationStart <code>STREAMING_QUERY_COMMAND</code> handleStreamingQueryCommand <code>STREAMING_QUERY_MANAGER_COMMAND</code> handleStreamingQueryManagerCommand <code>GET_RESOURCES_COMMAND</code> handleGetResourcesCommand <p><code>process</code> is used when:</p> <ul> <li><code>ExecuteThreadRunner</code> is requested to handle a command</li> </ul>"},{"location":"SparkConnectPlanner/#handleCommandPlugin","title":"handleCommandPlugin","text":"<pre><code>handleCommandPlugin(\n  extension: ProtoAny,\n  executeHolder: ExecuteHolder): Unit\n</code></pre> <p><code>handleCommandPlugin</code>...FIXME</p>"},{"location":"SparkConnectPlugin/","title":"SparkConnectPlugin","text":"<p><code>SparkConnectPlugin</code> is...FIXME</p>"},{"location":"SparkConnectServer/","title":"SparkConnectServer Standalone Application","text":"<p><code>SparkConnectServer</code> is a standalone application to start a Spark Connect server from command line.</p> <p><code>SparkConnectServer</code> can be started using <code>sbin/start-connect-server.sh</code> shell script.</p> <p><code>SparkConnectServer</code> can be stopped using <code>sbin/stop-connect-server.sh</code> shell script.</p>"},{"location":"SparkConnectServer/#main","title":"Launching Application","text":"<pre><code>main(\n  args: Array[String]): Unit\n</code></pre> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Starting Spark session.\n</code></pre> <p><code>main</code> creates a <code>SparkSession</code>.</p> <p><code>main</code> starts a SparkConnectService.</p> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Spark Connect server started.\n</code></pre> <p>In the end, <code>main</code> is paused until the SparkConnectService is terminated.</p>"},{"location":"SparkConnectService/","title":"SparkConnectService","text":"<p><code>SparkConnectService</code> is a <code>BindableService</code> (gRPC).</p> <p><code>SparkConnectService</code> can be started as a gRPC service using startGRPCService.</p>"},{"location":"SparkConnectService/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectService</code> takes the following to be created:</p> <ul> <li> <code>debug</code> flag <p><code>SparkConnectService</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to startGRPCService</li> </ul>"},{"location":"SparkConnectService/#start","title":"start","text":"<pre><code>start(\n  sc: SparkContext): Unit\n</code></pre> <p><code>start</code> starts the gRPC service and then creates a listener and the UI.</p> <p><code>start</code> is used when:</p> <ul> <li><code>SparkConnectPlugin</code> is requested for the Spark driver plugin</li> <li>SparkConnectServer standalone application is started</li> </ul>"},{"location":"SparkConnectService/#startGRPCService","title":"Start gRPC Service","text":"<pre><code>startGRPCService(): Unit\n</code></pre> <p><code>startGRPCService</code> reads the values of the following configuration properties:</p> Configuration Property Default Value <code>spark.connect.grpc.debug.enabled</code> <code>true</code> <code>spark.connect.grpc.binding.port</code> <code>15002</code> <code>spark.connect.grpc.maxInboundMessageSize</code> <code>128 * 1024 * 1024</code> <p><code>startGRPCService</code> builds a <code>NettyServerBuilder</code> with the <code>spark.connect.grpc.binding.port</code> and a SparkConnectService.</p> <p><code>startGRPCService</code> registers interceptors.</p> <p><code>startGRPCService</code> builds the server and starts it.</p>"},{"location":"SparkConnectService/#executePlan","title":"executePlan","text":"gRPC Java <pre><code>executePlan(\n  request: proto.ExecutePlanRequest,\n  responseObserver: StreamObserver[proto.ExecutePlanResponse]): Unit\n</code></pre> <p><code>executePlan</code> is part of the <code>SparkConnectServiceImplBase</code> abstraction.</p> <p><code>executePlan</code> creates a SparkConnectStreamHandler (with the given <code>StreamObserver</code>) to let it handle the request.</p>"},{"location":"SparkConnectStreamHandler/","title":"SparkConnectStreamHandler","text":""},{"location":"SparkConnectStreamHandler/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectStreamHandler</code> takes the following to be created:</p> <ul> <li> <code>StreamObserver</code> of <code>ExecutePlanResponse</code>s <p><code>SparkConnectStreamHandler</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to handle executePlan request</li> </ul>"},{"location":"SparkConnectStreamHandler/#handle","title":"Handling ExecutePlanRequest","text":"<pre><code>handle(\n  v: ExecutePlanRequest): Unit\n</code></pre> <p><code>handle</code>...FIXME</p>"},{"location":"SparkConnectStreamHandler/#handlePlan","title":"handlePlan","text":"<pre><code>handlePlan(\n  session: SparkSession,\n  request: ExecutePlanRequest): Unit\n</code></pre> <p><code>handlePlan</code>...FIXME</p>"},{"location":"logging/","title":"Logging","text":"<p>Spark Connect uses the same logging infrastructure as Apache Spark.</p>"},{"location":"overview/","title":"Spark Connect","text":"<p>Apache Spark 3.4 introduced Spark Connect module for a client-server interface for Apache Spark for remote connectivity to Spark clusters (using the DataFrame API and unresolved logical plans as the protocol based on gRPC Java).</p> <p>The Spark Connect server can be started using <code>sbin/start-connect-server.sh</code> shell script.</p> <pre><code>$ ./sbin/start-connect-server.sh\nstarting org.apache.spark.sql.connect.service.SparkConnectServer, logging to...\n\n$ tail -1 logs/spark-jacek-org.apache.spark.sql.connect.service.SparkConnectServer-1-Jaceks-Mac-mini.local.out\n... Spark Connect server started.\n</code></pre> <p>Use Spark Connect for interactive analysis:</p> <pre><code>./bin/pyspark --remote \"sc://localhost\"\n</code></pre> <p>And you will notice that the PySpark shell welcome message tells you that you have connected to Spark using Spark Connect:</p> <pre><code>Client connected to the Spark Connect server at localhost\n</code></pre> <p>Check the Spark session type:</p> <pre><code>SparkSession available as 'spark'.\n&gt;&gt;&gt; type(spark)\n&lt;class 'pyspark.sql.connect.session.SparkSession'&gt;\n</code></pre> <p>Now you can run PySpark code in the shell to see Spark Connect in action:</p> <pre><code>&gt;&gt;&gt; columns = [\"id\",\"name\"]\n&gt;&gt;&gt; data = [(1,\"Sarah\"),(2,\"Maria\")]\n&gt;&gt;&gt; df = spark.createDataFrame(data).toDF(*columns)\n&gt;&gt;&gt; df.show()\n+---+-----+\n| id| name|\n+---+-----+\n|  1|Sarah|\n|  2|Maria|\n+---+-----+\n</code></pre>"}]}